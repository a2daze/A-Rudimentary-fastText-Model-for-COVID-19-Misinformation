---
author: "Alex Zharchuk"
title: "Sentiment Analysis of COVID-19 Misinformation with FastText - Processing Data and Visualizing in R"
output: github_document
---

```{r}
library(tidyverse)
library(rvest)

#Read in data
disinformation <- read.csv('filtered_disinfo.csv')

```

```{r}

#Loop through every URL in the filtered list, extract text from paragraph nodes and write to a file
#TryCatch block is used to handle errors so that the script can keep running if
#There is a connection error
process_url <- function(url, label) {
  output_line <- paste(url, "Error fetching data", label, sep = ",")  # Default output assumes an error
  tryCatch({
    page <- read_html(url)
    text <- page %>%
      html_nodes('p') %>%
      html_text() %>%
      paste(collapse = " ")  # Collapse all paragraphs into one string

    output_line <- paste(url, text, label, sep = ",")
    print(paste("Processed:", url))
  }, error = function(e) {
    message("Error with site:", url, "- Error details:", e$message)
  })
  return(output_line)  # Return the output line whether it's the extracted text or the default error message
}

# Assuming you have a data frame 'disinformation' with columns 'urls' and 'labels'
results <- vector("character", nrow(disinformation))
for (i in seq_along(disinformation$Reported_On)) {
  results[i] <- process_url(disinformation$Reported_On[i], disinformation$Labels[i])
}
write_lines(results, "disinfo_train_labeled", append = TRUE)


# Loop through each row in your data frame
for (i in 1:nrow(disinformation)) {
  process_url(disinformation$Reported_On[i], disinformation$Label[i])
}
```




```{r}
ggplot(data=similarities, aes(word1,word2,fill=similarity))+
  geom_tile()+
  scale_fill_gradient(low='white',high='blue')+
  theme_minimal()+
  theme(axis.text.x=element_text(angle=45,hjust=1))+
  labs(title='Word Similarities')+
  xlab('Word 1')+ylab('Word 2')+ggtitle('Word Similarities') 
```

```{r}
library(tidyverse)
library(rvest)

select_test_urls<-test_sites$URL
select_test_urls
#Loop through every URL in the filtered list, extract text from paragraph nodes and write to a file
#TryCatch block is used to handle errors so that the script can keep running if
#There is a connection error

for (i in 1:length(select_test_urls)) {
  tryCatch({
    page <- read_html(select_test_urls[i])
    text <- page %>%
      html_nodes('p') %>%
      html_text()
      print(paste("Processing site", i, "with text length", length(text)))
      write_lines(text, "full_text_testsites_official",append=TRUE)
  }, error = function(e) {
    message("Error with site: ", select_test_urls[i], " - Skipping to next site.")
  })
}

write_csv(test_sites, 'test_sites.csv')

```

